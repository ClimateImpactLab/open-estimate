\documentclass[12pt, oneside]{amsart}
\usepackage[margin=1in]{geometry}
\geometry{letterpaper}
\usepackage[parfill]{parskip}
\usepackage{url}
\usepackage{graphicx}

\begin{document}

\section{Handling migration, crop indexes, and other combinations}

The problem that we have with some papers reporting bilateral
migration and others reporting net migration outflows is isomorphic to
the problem with crop indexes and demographically combined impacts:
responses are reported for linear combinations of coefficients, where
we are most interested in individual terms and have some estimates of
the individual terms.

We decompose the problem as follows.  Suppose that study $i$ reports
an estimate of the value $\beta_i$.  The underlying, disaggregated
coefficients are several values $\alpha_j$, and
$\beta_i = \sum_j a_{ij} \alpha_j$ for known coefficients $a_{ij}$.
Where study $i$ reports a fully disaggregated coefficient, $a_J = 1$
for some $J$ and $a_j = 0\,\,\,\forall\, j \ne J$.

In the case of migration, $j$ indexes all combinations of two
countries, $k \rightarrow l$.  Where only net migration flows are
reported, $a_{K \rightarrow l} = 1$ for a given $K$ and all $l$,
$a_{l \rightarrow K} = -1$, and all else are 0.

We allow that the reported coefficient are not the true coefficient
being estimated.  Let the reported coefficient of the true study
coefficient $\beta_i$ be $y_i$, with a standard error
$\sigma_i$.

\subsection{Full Pooling}

Under full pulling, the result is a multivariate normal distribution:
\begin{align*}
  p(\{\alpha_j\}) &\sim \mathcal{N}\left(\mu, \Sigma\right) \\
                  &\propto \prod_i \mathcal{N}\left(y_i | \sum_j a_{ij}
                    \alpha_j, \sigma_i\right) \\
\end{align*}

We can group terms together to get,
\begin{align*}
  \Sigma^{-1}_{jk} &= \sum_i \frac{a_{ij} a_{ik}}{\sigma_i^2} \\
  \mu &= \Sigma b \\
  b_j &= \sum_i \frac{y_i a_{ij}}{\sigma_i^2}
\end{align*}
which reduces to the normal pooling expression for disaggregated
coefficient estimates.

\subsubsection{The Regression Formulation}

The same outcome can be produced by a regression formulation, as
follows:
\[
\beta_i = \sum_j a_{ij} \alpha_j + \sigma_i \epsilon_i
\]
where $\epsilon_i \sim \mathcal{N}(0, 1)$.  We define $A =
\left(a_{ij}\right)$ and $\Sigma = diag\left(\frac{1}{\sigma_i^2}\right)$.

The solution to this is
\[
\hat\alpha = \left(A' \Sigma A\right)^{-1} A' \Sigma \beta
\]
which is equivalent to the solution above.

\subsection{Bayesian Hierarchical Model}

For Bayesian hierarchical modeling, we make a minor change to the
normal hierarchical modeling system to get
\begin{align*}
  y_i & \sim \mathcal{N}\left(\beta_i, \sigma_i\right) \\
  \beta_i & \sim \mathcal{N}\left(\sum_j a_{ij} \alpha_j, \sum_j a_{ij} \tau_j\right) \\
\end{align*}

The posterior distribution for this system follows
\[
p(\alpha, \tau, \beta | y_i) \propto
p(\alpha_j, \tau_j) \prod_i \mathcal{N}\left(\beta_i | \sum_j
  a_{ij} \alpha_j, \sum_j a_{ij} \tau_j\right)
\mathcal{N}\left(y_i | \beta_i, \sigma_i\right)
\]

This system can be solved in a direct sampling process similar to the normal
Bayesian hierarchical model.  We first define ``observation-specific''
hyper-variances:
\[
\nu_i = \sum_j a_{ij} \tau_j
\]

Now, taking the values of $\tau$ as given we can define
\begin{align*}
  p(\hat\beta_i | \tau) & \sim \mathcal{N}\left(y_i, \sigma_i^2 +
                              \nu_i^2\right) \\
  p(\alpha | \tau) & \sim \mathcal{N}\left(\hat\alpha, V\right)
\end{align*}

where
\begin{align*}
  \left(V^{-1}\right)_{jk} &= \sum_i \frac{a_{ij} a_{ik}}{\sigma_i^2 +
                             \nu_i^2} \\
  b_j &= \sum_i \frac{y_i a_{ij}}{\sigma_i^2 + \nu_i^2} \\
  \hat\alpha &= V b
\end{align*}

From these, the distribution of $\tau$ is exactly
\[
p(\tau | y) = p(\tau) \frac{p(\hat\beta | \tau)}{p(\alpha | \tau)}
\]

The posterior can be sampled by first sampling $\tau$ from its
posterior, and then applying that value to the posteriors for
$\hat\beta$ and $\alpha$.

\subsection{Combinations of estimates}

Whereas the sections above ask how one can get individual estimates
from various combinations, here we ask what the right way is to
combine component estimates.

\newpage
Let there be multiple studies, indexed by $i$ with the following
regression:
\[
y_i = X_i \beta_i + \epsilon_i
\]
and producing results:
\begin{align*}
  \hat\beta_i &= (X_i' X_i)^{-1} X_i' y \\
  \hat\Sigma_i &= \hat\sigma_i^2 (X_i' X_i)^{-1}
\end{align*}
with $\hat\sigma_i^2 = \frac{\hat\epsilon'\hat\epsilon}{n - k}$, as normal.

We now want to recover the result that would have been produced by
pooling the observations.
\[
\left(\begin{array}{c}
        y_1 \\
        y_2 \\
        \vdots
      \end{array}\right) = 
    \left(\begin{array}{c}
            X_1 \\
            X_2 \\
            \vdots
          \end{array}\right) \beta + \left(\begin{array}{c}
                                             \epsilon_1 \\
                                             \epsilon_2 \\
                                             \vdots
                                           \end{array}\right)
\]

The coefficients estimate is
\begin{align*}
\hat\beta &= \left(\sum_i X_i' X_i\right)^{-1}
            \left(\sum_i X_i' y_i\right) \\
          &= \left(\sum_i \hat\sigma_i^2 \hat\Sigma_i^{-1}\right)^{-1}
            \left(\sum_i \hat\sigma_i^2 \hat\Sigma_i^{-1} \hat\beta_i \right) \\
\end{align*}

Determining the variance-covariance matrix is difficult, because it is
not possible to recover $\hat\epsilon_i = y_i - X_i \hat\beta$.  However,
we can use a Feasible GLS-like approach, by taking the individual
estimates of $\sigma_i$ and estimating the HCE matrix:
\begin{align*}
  \hat\Sigma &= \left(\sum_i X_i' X_i\right)^{-1} \left(\sum_i \hat\sigma_i^2
               X_i' X_i\right) \left(\sum_i X_i' X_i\right)^{-1} \\
             &= \left(\sum_i \hat\Sigma_i / \hat\sigma_i^2\right)^{-1}
               \left(\sum_i \hat\Sigma_i \right) \left(\sum_i \hat\Sigma_i / \hat\sigma_i^2\right)^{-1}
\end{align*}

If the estimates share the same error distribution, this simplifies to
\[
\hat\Sigma = \sigma^4 \left(\sum_i \hat\Sigma_i\right)^{-1}
\]

\end{document}